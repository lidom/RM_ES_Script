\documentclass[xcolor=dvipsnames]{beamer}
%\usepackage[utf8]{inputenc}
%\usepackage{xcolor}
\usepackage{graphicx}
\usepackage{MnSymbol}
\usepackage{tikz}
\usepackage{media9}
\usetikzlibrary{arrows,shapes}
\usepackage{stmaryrd}
\usepackage{colortbl}
\usepackage{caption}
\usepackage{comment}
%\usepackage[utf8]{inputenc}
\usepackage[british,UKenglish,USenglish,english,american]{babel}
\usepackage{pdfpages}
\usepackage{listings}
\usepackage{color}
\usepackage{booktabs}
\usepackage{soul}
\usepackage[normalem]{ulem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tcolorbox}
\usepackage{lipsum}


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\usepackage{pgf}
\usepackage{etex}
\usepackage{tikz}
\usepackage{pgfplots}
\pgfplotsset{compat=1.15}

\tikzstyle{every picture}+=[remember picture]
% By default all math in TikZ nodes are set in inline mode. Change this to
% displaystyle so that we don't get small fractions.
\everymath{\displaystyle}


\usetheme{Antibes}
%\usetheme{Madrid}
%\usecolortheme[named=Maroon]{structure}
\usecolortheme{dolphin}
\usefonttheme{professionalfonts}
\useoutertheme{infolines}
\useinnertheme{circles}

\newtheorem*{bem}{Bemerkung}



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{listings}
\usepackage{color}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=tb,
  language=Java,
  aboveskip=2mm,
  belowskip=12mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=2
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title[RM Econometrics and Statistics]{Frisch-Waugh-Lovell Theorem}
\author[Caroline, Sofia, Max]{ \textbf{Research Module Econometrics and Statistics, \newline University of Bonn}}
\institute{}

\begin{document}

%%% 1
\begin{frame}
  \titlepage
\end{frame}

%%% 2
\begin{frame}
\frametitle{\textbf{Outline:}}
\tableofcontents
\end{frame}


%%% 2.1
\section{Context}
\begin{frame}{Context}
\begin{itemize}
    \item Interested in $\mathbf{X}_2$'s effect on $\mathbf{y}$ in bi-variate linear model
\end{itemize}
Under standard assumptions, classical linear model given by
\begin{align*}
  \mathbf{y} &= \mathbf{X}_1\mathbf{b}_1+\mathbf{X}_2\mathbf{b}_2+\hat{\boldsymbol{\varepsilon}}
\end{align*}
\pause
\begin{itemize}
    \item Frisch and Waugh (1933) offer alternative procedure
    \item Two-step procedure:
    \newline
    1. partial out effect of $\mathbf{X}_1$ on both $\mathbf{X}_2$ and $\mathbf{y}$ separately
    \newline
    2. regress residuals from $\mathbf{y}$ on $\mathbf{X}_1$ onto residuals from $\mathbf{y}$ on $\mathbf{X}_2$
    \newline
    ... inducing the following (partitioned) model
\end{itemize}
\begin{align*}
    \mathbf{y}-\hat{\mathbf{y}} =
    (\mathbf{X}_2-\hat{\mathbf{X}_2})\hat{\boldsymbol{\beta}}_2 +
    \hat{\mathbf{v}}
\end{align*}
with $\hat{\mathbf{y}}$ and $\hat{\mathbf{X}_2}$ depicting fitted values obtained from step 1.

\end{frame}



%%%
\begin{frame}{Frisch-Waugh-Lovell theorem}
\begin{itemize}
    \item Rewriting the partitioned model to
\end{itemize}
\begin{align*}
    \mathbf{M}_1\mathbf{y} =
    \mathbf{M}_1\mathbf{X}_2\hat{\boldsymbol{\beta}}_2+\hat{\mathbf{v}}
\end{align*}
with $\mathbf{M}_1 \equiv \mathbf{I}_n-\mathbf{X}_1(\mathbf{X}_1'\mathbf{X}_1)^{-1}\mathbf{X}_1' \equiv \mathbf{I}_n - \mathbf{P}_1$
\newline
\pause
\begin{itemize}
    \item Reminding ourselves of the classical linear model
\end{itemize}
\begin{align*}
  \mathbf{y} &= \mathbf{X}_1\mathbf{b}_1+\mathbf{X}_2\mathbf{b}_2+\hat{\boldsymbol{\varepsilon}}
\end{align*}
brings us to \textit{Proposition 3.1.5}

\begin{block}{\textit{Proposition 3.1.5 (Frisch-Waugh-Lovell theorem)}}
For the two underlying models, the following is true:
$\hat{\boldsymbol{\beta}}_2=\mathbf{b}_2\quad\text{and}\quad \hat{\boldsymbol{\varepsilon}}=\hat{\mathbf{v}}.$
\end{block}
\end{frame}


%%%
\section{Proof}
\begin{frame}\frametitle{\textit{Proof}}
\begin{itemize}
    \item First, prove that $\hat{\boldsymbol{\beta}}_2=\mathbf{b}_2$
    \item OLS estimators are given by
\end{itemize}
\begin{align*}
    \hat{\boldsymbol{\beta}}_2 &=
    \left(\left(\mathbf{M}_1 \mathbf{X}_2 \right)' \left(\mathbf{M}_1 \mathbf{X}_2 \right) \right)^{-1}
    \left(\mathbf{M}_1 \mathbf{X}_2 \right)' \mathbf{M}_1 \mathbf{y}\\
    &= \left(\mathbf{X}_2'\mathbf{M}_1\mathbf{X}_2\right)^{-1}\mathbf{X}_2'\mathbf{M}_1\mathbf{y}
    \nonumber \\
    \mathbf{b} &=(\mathbf{X}'\mathbf{X})^{-1}\mathbf{X}'\mathbf{y}
\end{align*}
\pause
\begin{itemize}
    \item Rewrite OLS estimator of classical linear model
\end{itemize}
\begin{align*}
    \mathbf{X}'\mathbf{X} \mathbf{b}&=\mathbf{X}'\mathbf{y}\\
    \\
    \left(\begin{matrix}\mathbf{X}_1'\mathbf{X}_1&\mathbf{X}_1'\mathbf{X}_2\\\mathbf{X}_2'\mathbf{X}_1&\mathbf{X}_2'\mathbf{X}_2\end{matrix}\right)\left(\begin{matrix}\mathbf{b}_1\\\mathbf{b}_2\end{matrix}\right)&=\left(\begin{matrix}\mathbf{X}_1'\mathbf{y}\\\mathbf{X}_2'\mathbf{y}\end{matrix}\right)
\end{align*}
\begin{itemize}
    \item System of two equations and two unknowns
\end{itemize}

\end{frame}


%%%
\begin{frame}\frametitle{\textit{Proof cont'd}}
\begin{itemize}
    \item Want to solve for $\mathbf{b}_2$
    \item In first equation write $\mathbf{b}_1$ as a function of $\mathbf{b}_2$
\end{itemize}

\begin{align*}
    \mathbf{b}_1 = \left(\mathbf{X}_1'\mathbf{X}_1\right)^{-1}\left(\mathbf{X}_1'\mathbf{y} - \mathbf{X}_1'\mathbf{X}_2\mathbf{b}_2\right)
\end{align*}
\begin{itemize}
    \item Plug this into second equation and obtain
\end{itemize}
\begin{align*}
    \mathbf{b}_2 &=\left(\mathbf{X}_2'\mathbf{M}_1\mathbf{X}_2\right)^{-1}\mathbf{X}_2'\mathbf{M}_1\mathbf{y}
\end{align*}
\begin{itemize}
    \item Remember OLS estimator of partitioned model
\end{itemize}
\begin{align*}
    \hat{\boldsymbol{\beta}}_2 &=
    \left(\mathbf{X}_2'\mathbf{M}_1\mathbf{X}_2\right)^{-1}\mathbf{X}_2'\mathbf{M}_1\mathbf{y}
\end{align*}
\end{frame}

%%%
\begin{frame}\frametitle{\textit{Proof cont'd}}
\begin{itemize}
    \item Now, want to show that $\hat{\boldsymbol{\varepsilon}}=\hat{\mathbf{v}}$
    \item From partitioned model we have
\end{itemize}
\begin{align*}
\hat{\mathbf{v}} &= \mathbf{M}_1\mathbf{y}-\mathbf{M}_1\mathbf{X}_2\hat{\boldsymbol{\beta}}_2
\end{align*}

\begin{align*}
    \hat{\boldsymbol{\varepsilon}}
    &= \mathbf{y}-\mathbf{X}_1{\color{red}{\mathbf{b}_1}}-\mathbf{X}_2\mathbf{b}_2 \\
    &= \mathbf{y}-\mathbf{X}_1{\color{red}{\left(\mathbf{X}_1'\mathbf{X}_1\right)^{-1}\left(\mathbf{X}_1'\mathbf{y}-\mathbf{X}_1'\mathbf{X}_2\mathbf{b}_2\right)}} -\mathbf{X}_2\mathbf{b}_2\\
    &= \mathbf{y}-\mathbf{P}_1\mathbf{y}-\left(\mathbf{X}_2\mathbf{b}_2-\mathbf{P}_1\mathbf{X}_2\mathbf{b}_2 \right)\\
    &= \mathbf{M}_1\mathbf{y}-\mathbf{M}_1\mathbf{X}_2\mathbf{b}_2\\
    &= \hat{\mathbf{v}}
\end{align*}

\end{frame}



%%%
\begin{frame}\frametitle{Conclusion}
\begin{itemize}
    \item Basic intuition 
\end{itemize}
A multivariate model with $K$ regressors can be broken into $K$ different bi-variate models \\
\bigskip
\begin{itemize}
    \item Usefulness of the theorem \\
\end{itemize}
 Sheds light on the \textit{machinery} of multivariate OLS \\
 Examples: understanding the problems of multicollinearity and omitted variable bias

\end{frame}

%%%
\section{Conclusion}
\begin{frame}\frametitle{Conclusion}
\begin{itemize}
    \item A classical application: de-trending a time series  
\end{itemize}
Time series with K explanatory variables and a linear time trend, $\mathbf{t = 1, 2, ...}$:
\begin{align*}
\mathbf{y}_t =\mathbf{b}_0 + \mathbf{dt} + \mathbf{X}_{1t}\mathbf{b}_1+\mathbf{X}_{2t}\mathbf{b}_2+...+\mathbf{X}_{kt}\mathbf{b}_{k}+\hat{\boldsymbol{\varepsilon}_{t}}
\end{align*}
\begin{itemize}
    \item Alternative to fitting by least squares:
    \newline
    1. partial out effect of time trend on $\mathbf{X}_{it}$ and $\mathbf{y}_{t}$ by regressing each on the time variable
    \newline
    2. use the residuals from these least squares regressions to calculate the de-trended variables
    \newline
    3. run the de-trended regression 
\end{itemize}
\end{frame}




\end{document}
