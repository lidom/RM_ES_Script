\documentclass{beamer}

\mode<presentation> {
\usetheme{Warsaw}
\usecolortheme{whale}
\setbeamercovered{transparent}
\usefonttheme{serif}
\newcommand*\oldmacro{}%
\let\oldmacro\insertshorttitle%
\renewcommand*\insertshorttitle{%
  \oldmacro\hfill%
  \insertframenumber\,/\,\inserttotalframenumber}
}
\newcommand*{\QEDB}{\hfill\ensuremath{\square}}
\DeclareRobustCommand{\bbone}{\text{\usefont{U}{bbold}{m}{n}1}}
\DeclareMathOperator{\EX}{\mathbb{E}}% expected value


\usepackage{amsmath}        % La base pour les maths

\title[Finite-Sample Properties of OLS]{Finite-Sample Properties of OLS}
\author[\textsc{Redmer, Modzelewski, Balaban}]{ \textsc{Raphael Redmer}, \textsc{Arkadiusz Modzelewski},\newline \textsc{Burak Balaban}}

\institute[TIPE]{University of Bonn \\ Research Module in Econometrics and Statistics}
\date{November 4, 2019}

\begin{document}


\begin{frame}
\titlepage % Print the title page as the first slide
\end{frame}

\begin{frame}
\frametitle{Overview} % Table of contents slide
\tableofcontents
\end{frame}



% Presentation of Theorem
\section{Introduction to the Theorem}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Theorem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{frame}
\frametitle{Introduction to the Theorem}
\begin{block}{Notation}
\begin{enumerate}

    \item    $y_{i}$ dependent variable,
    \item $ x_{ik}$ $ k$th independent variable (or regressor) with $k = 1,...,K$,
    \item $\epsilon_{i}$  stochastic error term,
    \item $i$ indexes the $i$th individual with $i = 1,...,n$, where $n$ is the sample size.
    
\end{enumerate}
\end{block}
\end{frame}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Next slide with assumptions required for theorem
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{frame}

\begin{block}{Assumptions}
\begin{enumerate}

    \item   <1->    Assumption 1.1 Linearity:  
    \newline
    $y_{i} = \sum\limits_{k=1}^K \beta_{k}x_{ik} + \epsilon_{i},    i = 1,...,n$
    \item   <2->    Assumption 1.2 Strict Exogeneity: 
    \newline
    $\EX(\epsilon_{i}|\textbf{X}) = 0$
    
    \item   <3->    Assumption 1.3 Rank Condition: 
    \newline
    $rank(\textbf{X}) = K$
    
    \item   <4->    Assumption 1.4 Spherical Error: 
    \newline
    $\EX(\epsilon_{i}^{2}|\textbf{X}) = \sigma^{2}$
    \newline
    $\EX(\epsilon_{i}\epsilon_{j}|\textbf{X}) = 0$, $i \neq j$
    
    
    
\end{enumerate}
\end{block}  
\end{frame}



\begin{frame}

\frametitle{Introduction to the Theorem}


\begin{block}{Theorem 4.1: Finite Sample Properties}
The OLS estimator $b$
\begin{enumerate}

	\item	<1->	is an unbiased estimator: $ \mathbb{E}(b|X) = \beta $
	
	\item	<2->	has variance: $ \mathbb{V}(b | X) = \sigma^{ 2 } (X'X)^{ -1 } $
	
	\item	<3->    has the lowest variance in the class of all linear unbiased estimators.
	
\end{enumerate}
\end{block}
\end{frame}

\begin{frame}{Explanation of $(3)$}
    For any unbiased estimator $\tilde{\mathbf{b}}$ that is linear in $\mathbf{y}$, we have:
    $\mathbb{V}(\tilde{\mathbf{b}}|\mathbf{X}) \geq \mathbb{V}(\mathbf{b} | \mathbf{X})$ in the matrix sense,
    \\meaning that\\
    $\mathbb{V}(\tilde{\mathbf{b}}|\mathbf{X}) - \mathbb{V}(\mathbf{b} | \mathbf{X}) = \mathbf{D}$, where $\mathbf{D}$ is
    a positive semidefinite $K\times K$ matrix, i.e.,
    $\mathbf{a}'\mathbf{D}\mathbf{a}\geq 0$ for any $K$-dimensional vector
    $\mathbf{a}$. 
    $\Rightarrow\mathbb{V}(\tilde{\text{b}}_k|\mathbf{X}) \geq \mathbb{V}({\textrm{b}}_k | \mathbf{X})$ for any
    $k=1,\dots,K$.\
 \end{frame}

% Title of the section
\section{Proof of Theorem}

% Proof (i)
\begin{frame}
\frametitle{Proof of $(1)$}
\title{Proof (1)}
\begin{align*}
    \onslide<1->{ \mathbb{E}(b | X) &= \mathbb{E}((X' X)^{-1} X' y | X) \\}
    \onslide<2->{ &= \mathbb{E}((X' X) ^ {-1} X' 
                     (X \beta +  \varepsilon )  | X) \\}
    \onslide<3->{ &= \mathbb{E}((X' X) ^ {-1} X' X  \beta 
                    + (X' X) ^ {-1} X' \varepsilon ) | X) \\}
    \onslide<4->{ &= \beta
                    + (X' X) ^ {-1} X' \underbrace{\mathbb{E}( \varepsilon  | X)}
                        _\text{$\underset{\mathrm{(A1.2)}}{ = } 0$} = \beta   \\ \QEDB}
\notag
\end{align*}
\end{frame}


% Proof (ii)
\begin{frame}
\frametitle{Proof of $(2)$}
\title{Proof (2)}
\begin{align*}
    \onslide<1->{ \mathbb{V}(b | X) &= \mathbb{V}(b - \beta | X) & \text{ ($\beta$ is not random) } \\
                    }
    \onslide<2->{ &= \mathbb{V}((X' X) ^ {-1} X' \varepsilon | X )) 
                    & \text{ ($b = \beta + (X' X) ^ {-1} X' \varepsilon$) } \\
                    }
    \onslide<3->{ &= (X' X) ^ {-1} X' 
                     \underbrace{\mathbb{V}( \varepsilon | X)}_\text{
                        $=\sigma^{2} I_{n}$ 
                     }
                     X( X' X) ^ {-1}  & \text{ (A1.4: Spherical Error)}\\
                    }
     \onslide<4->{ &= \sigma^{2}(X' X)^{-1}   \\ \QEDB}
\notag
\end{align*}
%\vskip-1.5em
\end{frame}


% Proof (iii)
% Assumption
\begin{frame}
\frametitle{Proof $(3)$}
\framesubtitle{Assumptions}
Let $\widetilde{b}$ be an unbiased estimator and linear in $y$ with
\begin{equation*}
  \widetilde{b} = Cy
\end{equation*}

where $C \in \mathbb{R}^{K \times n}$, which is a function of $X$ and/or nonrandom components 
(e.g. $C=(X' X) ^ {-1} X'$ as in OLS Model).

%\vskip-1.5em
\end{frame}


% % Decomposition
\begin{frame}
\frametitle{Proof of $(3)$}
\framesubtitle{Decomposition}
\title{Proof (3)}
With $D := C -(X' X)^{-1} X'$, $\widetilde{b}$ can be decomposed to:
\begin{align*}
    \onslide<1->{ \widetilde{b} &= Cy = (C \overbrace{ - (X' X) ^ {-1} X' + (X' X)^ {-1}                                     X'}^\text{ $= 0$ } )y   \\}
    \onslide<2->{ &= Dy + \underbrace{ (X' X)^ {-1} X' y }_\text{$=b$}\\}
    \onslide<3->{ &= D(X\beta + \varepsilon) + b\\}
    \onslide<3->{ &= DX\beta + D\varepsilon + b \\}
\notag
\end{align*}
%\vskip-1.5em
\end{frame}

\begin{frame}
\frametitle{Proof $(3)$}
\framesubtitle{$\widetilde{b}$ unbiased}
Due to $\widetilde{b}$ being unbiased, it follows that $\mathbb{E}(\widetilde{b} | X) = \beta$. Therefore, 
\begin{align*}
    \onslide<1->{ \mathbb{E}( \widetilde{b} | X) &=  \mathbb{E}(  DX\beta | X) 
                                                    + \mathbb{E}( D\varepsilon | X) + \mathbb{E}( b | X) \\}
    \onslide<2->{ &= DX\beta + 0 + \beta = \beta \\
                    }
    \onslide<3->{ &\Longrightarrow DX = 0 \\}
    \onslide<4->{ \Longrightarrow \widetilde{b} &= D\varepsilon + b  \\}
    \onslide<5->{ \Longleftrightarrow \widetilde{b} - \beta &= D\varepsilon +  (b - \beta) \\}
    \onslide<6->{ &= D\varepsilon +  (X' X) ^ {-1} X' \varepsilon \\}
    \onslide<7->{ &= (D +  (X' X) ^ {-1} X' )\varepsilon  & \text{ ($*$) } \\}
\notag
\end{align*}
%\vskip-1.5em
\end{frame}

\begin{frame}
\frametitle{Proof $(3)$}
\framesubtitle{Rearrangement}
By using ($*$), $\mathbb{V}(\widetilde{b} | X)$ can now be rearranged such that:
\begin{align*}
    \onslide<1->{ \mathbb{V}(\widetilde{b} | X) &= \mathbb{V}(\widetilde{b} - \beta | X) \\
                    }
    \onslide<2->{ &\overset{\mathrm{(*)}}{ = } \mathbb{V}( (D + (X' X) ^ {-1} X' )\varepsilon | X ) \\
                    }
    \onslide<3->{ &= (D + (X' X) ^ {-1} X' ) 
                    \underbrace{\mathbb{V}(\varepsilon | X)}_\text{$=\sigma^{2}I_{n}$}
                    (D' + X(X' X) ^ {-1}) \\
                    }                    
    \onslide<4->{ &\overset{\mathrm{(DX = 0)}}{ = } \sigma^{2}(D D' + (X' X) ^ {-1}) \\
                    }
    \onslide<5->{ &\geq \sigma^{2}((X' X) ^ {-1}) = \mathbb{V}(b | X),}
\notag
\end{align*}
\onslide<5->{since $DD'$ is positive semidefinite. 
\QEDB}
%\vskip-1.5em
\end{frame}

\section{Conclusions}

\begin{frame}
\frametitle{Conclusions}

\begin{itemize}
    \item   Under stated assumptions OLS estimator has very nice properties
    \item   Assumptions in the real life examples are very difficult to satisfy. Example: model with lagged dependent variable
    \newline
    
    \item  Team work: communication is more important than you think.
\end{itemize}

\end{frame}

\end{document}