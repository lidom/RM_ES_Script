<!DOCTYPE html>
<html lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Ch. 2 Estimation Theory | Research Module in Econometrics &amp; Statistics</title>
  <meta name="description" content="Script for the research module in econometrics &amp; statistics (University Bonn)." />
  <meta name="generator" content="bookdown 0.24 and GitBook 2.6.7" />

  <meta property="og:title" content="Ch. 2 Estimation Theory | Research Module in Econometrics &amp; Statistics" />
  <meta property="og:type" content="book" />
  <meta property="og:url" content="http://dliebl.com/RM_ES_Script/" />
  
  <meta property="og:description" content="Script for the research module in econometrics &amp; statistics (University Bonn)." />
  <meta name="github-repo" content="lidom/RM_ES_Script" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Ch. 2 Estimation Theory | Research Module in Econometrics &amp; Statistics" />
  
  <meta name="twitter:description" content="Script for the research module in econometrics &amp; statistics (University Bonn)." />
  

<meta name="author" content="Prof.Â Dominik Liebl (dliebl@uni-bonn.de) and Christopher Walsh (cwalsh@uni-bonn.de)" />


<meta name="date" content="2021-11-09" />

  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="introduction-to-r.html"/>
<link rel="next" href="statistical-hypothesis-testing.html"/>
<script src="libs/header-attrs-2.11/header-attrs.js"></script>
<script src="libs/jquery-3.6.0/jquery-3.6.0.min.js"></script>
<script src="https://cdn.jsdelivr.net/npm/fuse.js@6.4.6/dist/fuse.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-clipboard.css" rel="stylesheet" />








<link href="libs/anchor-sections-1.0.1/anchor-sections.css" rel="stylesheet" />
<script src="libs/anchor-sections-1.0.1/anchor-sections.js"></script>


<style type="text/css">
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  background-color: #f8f8f8; }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
code span.al { color: #ef2929; } /* Alert */
code span.an { color: #8f5902; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #c4a000; } /* Attribute */
code span.bn { color: #0000cf; } /* BaseN */
code span.cf { color: #204a87; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4e9a06; } /* Char */
code span.cn { color: #000000; } /* Constant */
code span.co { color: #8f5902; font-style: italic; } /* Comment */
code span.cv { color: #8f5902; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #8f5902; font-weight: bold; font-style: italic; } /* Documentation */
code span.dt { color: #204a87; } /* DataType */
code span.dv { color: #0000cf; } /* DecVal */
code span.er { color: #a40000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #0000cf; } /* Float */
code span.fu { color: #000000; } /* Function */
code span.im { } /* Import */
code span.in { color: #8f5902; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #204a87; font-weight: bold; } /* Keyword */
code span.op { color: #ce5c00; font-weight: bold; } /* Operator */
code span.ot { color: #8f5902; } /* Other */
code span.pp { color: #8f5902; font-style: italic; } /* Preprocessor */
code span.sc { color: #000000; } /* SpecialChar */
code span.ss { color: #4e9a06; } /* SpecialString */
code span.st { color: #4e9a06; } /* String */
code span.va { color: #000000; } /* Variable */
code span.vs { color: #4e9a06; } /* VerbatimString */
code span.wa { color: #8f5902; font-weight: bold; font-style: italic; } /* Warning */
</style>

<style type="text/css">
/* Used with Pandoc 2.11+ new --citeproc when CSL is used */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><a href="http://www.dliebl.com/RM_ES_Script/">Research Module E&S</a></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Preface</a></li>
<li class="chapter" data-level="" data-path="topics.html"><a href="topics.html"><i class="fa fa-check"></i>Topics</a></li>
<li class="chapter" data-level="1" data-path="introduction-to-r.html"><a href="introduction-to-r.html"><i class="fa fa-check"></i><b>1</b> Introduction to R</a>
<ul>
<li class="chapter" data-level="1.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#short-glossary"><i class="fa fa-check"></i><b>1.1</b> Short Glossary</a></li>
<li class="chapter" data-level="1.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#first-steps"><i class="fa fa-check"></i><b>1.2</b> First Steps</a></li>
<li class="chapter" data-level="1.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#further-data-objects"><i class="fa fa-check"></i><b>1.3</b> Further Data Objects</a></li>
<li class="chapter" data-level="1.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#simple-regression-analysis-using-r"><i class="fa fa-check"></i><b>1.4</b> Simple Regression Analysis using R</a></li>
<li class="chapter" data-level="1.5" data-path="introduction-to-r.html"><a href="introduction-to-r.html#programming-in-r"><i class="fa fa-check"></i><b>1.5</b> Programming in R</a></li>
<li class="chapter" data-level="1.6" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-packages"><i class="fa fa-check"></i><b>1.6</b> R-packages</a></li>
<li class="chapter" data-level="1.7" data-path="introduction-to-r.html"><a href="introduction-to-r.html#tidyverse"><i class="fa fa-check"></i><b>1.7</b> Tidyverse</a>
<ul>
<li class="chapter" data-level="1.7.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#tidyverse-plotting-basics"><i class="fa fa-check"></i><b>1.7.1</b> Tidyverse: Plotting Basics</a></li>
<li class="chapter" data-level="1.7.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#tidyverse-data-wrangling-basics"><i class="fa fa-check"></i><b>1.7.2</b> Tidyverse: Data Wrangling Basics</a></li>
<li class="chapter" data-level="1.7.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#the-pipe-operator"><i class="fa fa-check"></i><b>1.7.3</b> The pipe operator <code>%&gt;%</code></a></li>
<li class="chapter" data-level="1.7.4" data-path="introduction-to-r.html"><a href="introduction-to-r.html#the-group_by-function"><i class="fa fa-check"></i><b>1.7.4</b> The <code>group_by()</code> function</a></li>
</ul></li>
<li class="chapter" data-level="1.8" data-path="introduction-to-r.html"><a href="introduction-to-r.html#further-links"><i class="fa fa-check"></i><b>1.8</b> Further Links</a>
<ul>
<li class="chapter" data-level="1.8.1" data-path="introduction-to-r.html"><a href="introduction-to-r.html#further-r-intros"><i class="fa fa-check"></i><b>1.8.1</b> Further R-Intros</a></li>
<li class="chapter" data-level="1.8.2" data-path="introduction-to-r.html"><a href="introduction-to-r.html#version-control-gitgithub"><i class="fa fa-check"></i><b>1.8.2</b> Version Control (Git/GitHub)</a></li>
<li class="chapter" data-level="1.8.3" data-path="introduction-to-r.html"><a href="introduction-to-r.html#r-ladies"><i class="fa fa-check"></i><b>1.8.3</b> R-Ladies</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="2" data-path="estimation-theory.html"><a href="estimation-theory.html"><i class="fa fa-check"></i><b>2</b> Estimation Theory</a>
<ul>
<li class="chapter" data-level="2.1" data-path="estimation-theory.html"><a href="estimation-theory.html#bias-variance-and-mse"><i class="fa fa-check"></i><b>2.1</b> Bias, Variance and MSE</a></li>
<li class="chapter" data-level="2.2" data-path="estimation-theory.html"><a href="estimation-theory.html#consistency-of-estimators"><i class="fa fa-check"></i><b>2.2</b> Consistency of Estimators</a></li>
<li class="chapter" data-level="2.3" data-path="estimation-theory.html"><a href="estimation-theory.html#rates-of-convergence"><i class="fa fa-check"></i><b>2.3</b> Rates of Convergence</a></li>
<li class="chapter" data-level="2.4" data-path="estimation-theory.html"><a href="estimation-theory.html#asymptotic-distributions"><i class="fa fa-check"></i><b>2.4</b> Asymptotic Distributions</a></li>
<li class="chapter" data-level="2.5" data-path="estimation-theory.html"><a href="estimation-theory.html#asymptotic-theory"><i class="fa fa-check"></i><b>2.5</b> Asymptotic Theory</a></li>
<li class="chapter" data-level="2.6" data-path="estimation-theory.html"><a href="estimation-theory.html#mathematical-tools"><i class="fa fa-check"></i><b>2.6</b> Mathematical tools</a>
<ul>
<li class="chapter" data-level="2.6.1" data-path="estimation-theory.html"><a href="estimation-theory.html#taylor-expansions"><i class="fa fa-check"></i><b>2.6.1</b> Taylor expansions</a></li>
<li class="chapter" data-level="2.6.2" data-path="estimation-theory.html"><a href="estimation-theory.html#tools-for-deriving-asymptotic-distributions"><i class="fa fa-check"></i><b>2.6.2</b> Tools for deriving asymptotic distributions</a></li>
<li class="chapter" data-level="2.6.3" data-path="estimation-theory.html"><a href="estimation-theory.html#the-delta-method"><i class="fa fa-check"></i><b>2.6.3</b> The Delta-Method</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="3" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html"><i class="fa fa-check"></i><b>3</b> Statistical Hypothesis Testing</a>
<ul>
<li class="chapter" data-level="3.1" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html#hypotheses-and-test-statistics"><i class="fa fa-check"></i><b>3.1</b> Hypotheses and Test-Statistics</a></li>
<li class="chapter" data-level="3.2" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html#significance-level-size-and-p-values"><i class="fa fa-check"></i><b>3.2</b> Significance Level, Size and p-Values</a></li>
<li class="chapter" data-level="3.3" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html#PF1"><i class="fa fa-check"></i><b>3.3</b> The Power Function</a></li>
<li class="chapter" data-level="3.4" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html#asymptotic-null-distributions"><i class="fa fa-check"></i><b>3.4</b> Asymptotic Null Distributions</a></li>
<li class="chapter" data-level="3.5" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html#multiple-comparisons"><i class="fa fa-check"></i><b>3.5</b> Multiple Comparisons</a></li>
<li class="chapter" data-level="3.6" data-path="statistical-hypothesis-testing.html"><a href="statistical-hypothesis-testing.html#r-lab-the-gauss-test"><i class="fa fa-check"></i><b>3.6</b> R-Lab: The Gauss-Test</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html"><i class="fa fa-check"></i><b>4</b> Ordinary Least Squares: The Classical Linear Regression Model</a>
<ul>
<li class="chapter" data-level="4.1" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#finite-sample-properties"><i class="fa fa-check"></i><b>4.1</b> Finite-Sample Properties</a>
<ul>
<li class="chapter" data-level="4.1.1" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#the-algebra-of-least-squares"><i class="fa fa-check"></i><b>4.1.1</b> The Algebra of Least Squares</a></li>
<li class="chapter" data-level="" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#some-quantities-of-interest"><i class="fa fa-check"></i>Some quantities of interest:</a></li>
<li class="chapter" data-level="4.1.2" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#coefficient-of-determination"><i class="fa fa-check"></i><b>4.1.2</b> Coefficient of determination</a></li>
<li class="chapter" data-level="" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#partitioned-regression-model"><i class="fa fa-check"></i>Partitioned regression model</a></li>
<li class="chapter" data-level="4.1.3" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#finite-sample-properties-of-ols"><i class="fa fa-check"></i><b>4.1.3</b> Finite-Sample Properties of OLS</a></li>
<li class="chapter" data-level="4.1.4" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#Testing"><i class="fa fa-check"></i><b>4.1.4</b> Hypothesis Testing under Normality</a></li>
</ul></li>
<li class="chapter" data-level="4.2" data-path="ordinary-least-squares-the-classical-linear-regression-model.html"><a href="ordinary-least-squares-the-classical-linear-regression-model.html#asymptotics-under-the-classic-regression-model"><i class="fa fa-check"></i><b>4.2</b> Asymptotics under the Classic Regression Model</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="monte-carlo-simulations.html"><a href="monte-carlo-simulations.html"><i class="fa fa-check"></i><b>5</b> Monte-Carlo Simulations</a>
<ul>
<li class="chapter" data-level="5.1" data-path="monte-carlo-simulations.html"><a href="monte-carlo-simulations.html#checking-test-statistics"><i class="fa fa-check"></i><b>5.1</b> Checking Test Statistics</a>
<ul>
<li class="chapter" data-level="5.1.1" data-path="monte-carlo-simulations.html"><a href="monte-carlo-simulations.html#simple-example-gauss-test"><i class="fa fa-check"></i><b>5.1.1</b> Simple Example: Gauss Test</a></li>
<li class="chapter" data-level="5.1.2" data-path="monte-carlo-simulations.html"><a href="monte-carlo-simulations.html#simulated-power-function"><i class="fa fa-check"></i><b>5.1.2</b> Simulated Power Function</a></li>
</ul></li>
<li class="chapter" data-level="5.2" data-path="monte-carlo-simulations.html"><a href="monte-carlo-simulations.html#checking-parameter-estimators"><i class="fa fa-check"></i><b>5.2</b> Checking Parameter Estimators</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="how-to-write.html"><a href="how-to-write.html"><i class="fa fa-check"></i><b>6</b> How to Write</a>
<ul>
<li class="chapter" data-level="6.1" data-path="how-to-write.html"><a href="how-to-write.html#five-common-writing-mistakes"><i class="fa fa-check"></i><b>6.1</b> Five Common Writing Mistakes</a></li>
<li class="chapter" data-level="6.2" data-path="how-to-write.html"><a href="how-to-write.html#gregory-mankiw-how-to-write-well"><i class="fa fa-check"></i><b>6.2</b> Gregory Mankiw: How to Write Well</a></li>
<li class="chapter" data-level="6.3" data-path="how-to-write.html"><a href="how-to-write.html#rob-j-hyndman-avoid-annoying-a-referee"><i class="fa fa-check"></i><b>6.3</b> Rob J Hyndman: Avoid Annoying a Referee</a></li>
<li class="chapter" data-level="6.4" data-path="how-to-write.html"><a href="how-to-write.html#writing-tips-by-john-h.-cochrane"><i class="fa fa-check"></i><b>6.4</b> Writing Tips by John H. Cochrane</a></li>
<li class="chapter" data-level="6.5" data-path="how-to-write.html"><a href="how-to-write.html#latex"><i class="fa fa-check"></i><b>6.5</b> LaTeX</a></li>
<li class="chapter" data-level="6.6" data-path="how-to-write.html"><a href="how-to-write.html#general-paper-structure"><i class="fa fa-check"></i><b>6.6</b> General Paper Structure</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="how-to-present.html"><a href="how-to-present.html"><i class="fa fa-check"></i><b>7</b> How to Present</a>
<ul>
<li class="chapter" data-level="7.1" data-path="how-to-present.html"><a href="how-to-present.html#the-aim-of-your-talk"><i class="fa fa-check"></i><b>7.1</b> The Aim of your Talk</a></li>
<li class="chapter" data-level="7.2" data-path="how-to-present.html"><a href="how-to-present.html#a-suggested-structure"><i class="fa fa-check"></i><b>7.2</b> A Suggested Structure</a></li>
<li class="chapter" data-level="7.3" data-path="how-to-present.html"><a href="how-to-present.html#preparing-slides"><i class="fa fa-check"></i><b>7.3</b> Preparing Slides</a></li>
<li class="chapter" data-level="7.4" data-path="how-to-present.html"><a href="how-to-present.html#keeping-to-time"><i class="fa fa-check"></i><b>7.4</b> Keeping to Time</a></li>
<li class="chapter" data-level="7.5" data-path="how-to-present.html"><a href="how-to-present.html#giving-the-presentation"><i class="fa fa-check"></i><b>7.5</b> Giving the Presentation</a></li>
<li class="chapter" data-level="" data-path="how-to-present.html"><a href="how-to-present.html#final-advice"><i class="fa fa-check"></i>Final Advice</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i>References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Research Module in Econometrics &amp; Statistics</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="estimation-theory" class="section level1" number="2">
<h1><span class="header-section-number">Ch. 2</span> Estimation Theory</h1>
<div id="bias-variance-and-mse" class="section level2" number="2.1">
<h2><span class="header-section-number">2.1</span> Bias, Variance and MSE</h2>
<p>Given a sample <span class="math inline">\(X_1,\dots,X_n\)</span> consider an estimator <span class="math inline">\(\widehat{\theta}_n\equiv\widehat{\theta}(X_1,\dots,X_n)\)</span> of a real-valued parameter <span class="math inline">\(\theta\in\Omega\subset\mathbb{R}\)</span>.</p>
<p>The distribution of any estimator of course depends on the true parameter vector <span class="math inline">\(\theta\)</span>, i.e., more precisely, <span class="math display">\[\widehat{\theta}_n\equiv \widehat{\theta}(X_1,\dots,X_n;\theta).\]</span>
This dependence is usually not explicitly written, but all properties of estimators discussed below have to hold for all possible parameter values <span class="math inline">\(\theta\in\Omega\)</span>. This will go without saying.</p>
<p><br />
</p>
<p>Statistical inference requires to assess the accuracy of an estimator.</p>
<p><br />
</p>
<p>The <strong>bias</strong> of an estimator is defined by
<span class="math display">\[\textrm{Bias}(\widehat{\theta}_n)=E(\widehat{\theta}_n)-\theta\]</span>
An estimator is called <strong>unbiased</strong> if <span class="math inline">\(E(\widehat{\theta}_n)=\theta\)</span> and hence <span class="math inline">\(\textrm{Bias}(\widehat{\theta}_n)=0\)</span> (for all possible <span class="math inline">\(\theta\in\Omega\)</span>).</p>
<p><br />
</p>
<p>The <strong>variance</strong> of an estimator is given by
<span class="math display">\[\textrm{var}(\widehat{\theta}_n)=E\left((\widehat{\theta}_n-E(\widehat{\theta}_n))^2\right).\]</span></p>
<p><br />
</p>
<p>Performance of an estimator is most frequently evaluated with respect to the <strong>quadratic loss</strong> (also
called <span class="math inline">\(L_2\)</span> loss)
<span class="math display">\[(\widehat{\theta}_n-\theta)^2.\]</span>
The corresponding risk is the <strong>Mean Squared Error (MSE)</strong>
<span class="math display">\[E\left((\widehat{\theta}_n-\theta)^2\right)=\textrm{Bias}(\widehat{\theta}_n)^2+\textrm{var}(\widehat{\theta}_n)\]</span>
For an unbiased estimator the mean squared error is obviously equal to the variance of the estimator.</p>
<p><br />
</p>
<p><strong>Example:</strong> Assume an i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span> with mean <span class="math inline">\(\mu=E(X_i)\)</span> and variance <span class="math inline">\(\sigma^2=\textrm{var}(X_i)&lt;\infty\)</span>.</p>
<ul>
<li>The sample mean <span class="math inline">\(\bar X\)</span> is an unbiased estimator of the true mean <span class="math inline">\(\mu\)</span>, since the equation
<span class="math display">\[E(\bar X)=\mu\]</span>
holds for any possible value of the true mean <span class="math inline">\(\mu\)</span>.</li>
<li>The variance of the estimator <span class="math inline">\(\bar X\)</span> is given by
<span class="math display">\[\textrm{var}(\bar X)=\sigma^2/n\]</span></li>
<li>The mean squared error of the estimator <span class="math inline">\(\bar X\)</span> is given by
<span class="math display">\[E\left((\bar X-\mu)^2\right)=\textrm{var}(\bar X)=\sigma^2/n\]</span></li>
</ul>
</div>
<div id="consistency-of-estimators" class="section level2" number="2.2">
<h2><span class="header-section-number">2.2</span> Consistency of Estimators</h2>
<p>Asymptotic theory is concerned with theoretical results valid for âlarge sample sizesâ. Important keywords of asymptotic theory are:</p>
<ul>
<li>consistency</li>
<li>rates of convergence</li>
<li>asymptotic distributions</li>
</ul>
<p>They all rely on elaborated concepts on the stochastic convergence of random variables.</p>
<p><br />
</p>
<p><strong>Stochastic convergence.</strong>
Let <span class="math inline">\(\{Z_n\}_{n=1,2,3,\dots}\)</span> be a sequence of <strong>random variables</strong>. Mathematically, there are different kinds of convergence of
<span class="math inline">\(\{Z_n\}\)</span> to a fixed value <span class="math inline">\(c\)</span>. The three most important are:</p>
<ul>
<li>Convergence in probability (abbreviated <span class="math inline">\(Z_n\to_P c\)</span>):
<span class="math display">\[\lim_{n\to\infty} P\left(|Z_n-c|&gt;\epsilon\right)=0\quad\hbox{ for all }\quad\epsilon&gt;0\]</span></li>
<li>Almost sure convergence (abbreviated <span class="math inline">\(Z_n\to_{a.s.} c\)</span>):
<span class="math display">\[P\left(\lim_{n\to\infty} Z_n=c\right)=1\]</span></li>
<li>Convergence in quadratic mean (abbreviated <span class="math inline">\(Z_n\to_{q.m.} c\)</span>):
<span class="math display">\[\lim_{n\to\infty} E\left((Z_n-c)^2\right)=0\]</span></li>
</ul>
<p>Note that:</p>
<ul>
<li><span class="math inline">\(Z_n\to_{a.s.} c\)</span> implies <span class="math inline">\(Z_n\to_P c\)</span></li>
<li><span class="math inline">\(Z_n\to_{q.m.} c\)</span> implies <span class="math inline">\(Z_n\to_P c\)</span></li>
</ul>
<p><br />
</p>
<p><strong>Consistency of estimators.</strong> Based on a sample <span class="math inline">\(X_1,\dots,X_n\)</span> let <span class="math inline">\(\hat\theta_n\equiv\theta_n(X_1,\dots,X_n)\)</span> be an estimator of an unknown parameter <span class="math inline">\(\theta\)</span>.</p>
<ul>
<li><span class="math inline">\(\hat\theta_n\)</span> is called âweakly consistentâ if
<span class="math display">\[\hat{\theta}_n\to_{P} \theta\quad \hbox{ as }\quad n\to\infty \]</span></li>
<li><span class="math inline">\(\hat\theta_n\)</span> is called âstrongly consistentâ if
<span class="math display">\[\hat{\theta}_n\to_{a.s.} \theta\quad \hbox{ as }\quad n\to\infty \]</span></li>
</ul>
<p><br />
</p>
<p><strong>Remark:</strong> For most statistical estimation problems it is usually possible to define many different estimators. The real problem is to find a good estimator which approximates the true parameter <span class="math inline">\(\theta\)</span> with the maximal possible accuracy. Consistency is generally seen as a necessary condition which has to be satisfied by any reasonable estimator. In econometric practice usually only weak consistency is derived which generally follows from weak <a href="https://www.statlect.com/asymptotic-theory/law-of-large-numbers"><strong>laws of large numbers</strong></a>.</p>
<p><br />
</p>
<p><strong>Example:</strong> Assume again an i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span> with mean <span class="math inline">\(\mu=E(X_i)\)</span> and variance <span class="math inline">\(\sigma^2=\textrm{var}(X_i)&lt;\infty\)</span>. As stated above we then have
<span class="math display">\[E\left((\bar X-\mu)^2\right)=\textrm{var}(\bar X)=\sigma^2/n\rightarrow 0 \quad \text{as } n\rightarrow\infty.\]</span>
Therefore, <span class="math inline">\(\bar X \to_{q.m.} \mu\)</span>. The latter implies that <span class="math inline">\(\bar X \to_{P} \mu\)</span>, i.e.Â <span class="math inline">\(\bar X\)</span> is a (weakly) consistent estimator of <span class="math inline">\(\mu\)</span>.</p>
</div>
<div id="rates-of-convergence" class="section level2" number="2.3">
<h2><span class="header-section-number">2.3</span> Rates of Convergence</h2>
<p>Rates of convergence quantify the (stochastic) order of magnitude of an estimation error in dependence of the
sample size <span class="math inline">\(n\)</span>. This order of magnitude is usually represented using the symbols: <span class="math inline">\(O_P\)</span> and <span class="math inline">\(o_P\)</span>.</p>
<p><br />
</p>
<p>Let <span class="math inline">\(\{Z_n\}_{n=1,2,3,\dots}\)</span> be a sequence of random variables, and let <span class="math inline">\(\{c_n\}_{n=1,2,3,\dots}\)</span> be a sequence of positive (deterministic) numbers.
<!-- ($c_n$ may de a deterministic sequence of real numbers or it may be a sequence of random variables) --></p>
<ul>
<li>We will write <span class="math inline">\(Z_n=O_p(c_n)\)</span> if for any <span class="math inline">\(\epsilon&gt;0\)</span> there exist numbers <span class="math inline">\(0&lt;M&lt;\infty\)</span> and <span class="math inline">\(m\)</span> such that
<span class="math display">\[P(|Z_n|\ge M\cdot c_n)\leq\epsilon\quad\hbox{ for all }\quad n\geq m.\]</span></li>
<li>We will write <span class="math inline">\(Z_n=o_p(c_n)\)</span> if
<span class="math display">\[\lim_{n\to\infty} P(|Z_n|\geq\epsilon\cdot c_n)=0\quad\hbox{ for all }\quad \epsilon&gt;0.\]</span></li>
<li>With <span class="math inline">\(c_n=1\)</span> for all <span class="math inline">\(n\)</span>, <span class="math inline">\(Z_n=O_p(1)\)</span> means that the sequence <span class="math inline">\(\{Z_n\}\)</span> is <strong>stochastically bounded</strong>. I.e., for any <span class="math inline">\(\epsilon&gt;0\)</span> there exist number <span class="math inline">\(0&lt;M&lt;\infty\)</span> and <span class="math inline">\(m\)</span> such that
<span class="math display">\[P(|Z_n|\geq M)\leq\epsilon\quad\hbox{ for all }\quad n\geq m.\]</span></li>
<li>With <span class="math inline">\(c_n=1\)</span> for all <span class="math inline">\(n\)</span>, <span class="math inline">\(Z_n=o_P(1)\)</span> is equivalent to <span class="math inline">\(Z_n\to_{P} 0\)</span>, i.e., <span class="math inline">\(Z_n\)</span> <strong>converges in probability to zero</strong>.</li>
</ul>
<p>Note that:</p>
<ul>
<li><span class="math inline">\(Z_n=O_p(c_n)\)</span> is equivalent to <span class="math inline">\(Z_n/c_n=O_p(1)\)</span></li>
<li><span class="math inline">\(Z_n=o_p(c_n)\)</span> is equivalent to <span class="math inline">\(Z_n/c_n=o_p(1)\)</span></li>
</ul>
<p><br />
</p>
<p><strong>Definition:</strong> An estimator <span class="math inline">\(\hat\theta\equiv\hat\theta_n\)</span> of a parameter <span class="math inline">\(\theta\)</span> possesses the
<strong>rate of convergence</strong> <span class="math inline">\(n^{-r}\)</span> if and only if <span class="math inline">\(r\)</span> is the <em>largest positive number</em> with the property that
<span class="math display">\[|\hat\theta_n-\theta|=O_P(n^{-r}).\]</span></p>
<p>The rate of convergence quantifies how fast the estimation error decreases when increasing the sample size <span class="math inline">\(n\)</span>.</p>
<p><br />
</p>
<p><strong>Unbiased estimators:</strong> Let <span class="math inline">\(\hat\theta_n\)</span> be an <em>unbiased</em> estimator of an unknown parameter <span class="math inline">\(\theta\)</span> satisfying <span class="math inline">\(\textrm{var}(\hat\theta_n)=C n^{-1}\)</span> for some <span class="math inline">\(0&lt;C&lt;\infty\)</span>. Then <span class="math inline">\(\hat\theta_n\)</span> possesses the rate of convergence <span class="math inline">\(n^{-1/2}\)</span>. This is a consequence of the <a href="https://www.statlect.com/fundamentals-of-probability/Chebyshev-inequality">Chebyshev inequality</a>.</p>
<p><br />
</p>
<p><strong>Chebyshev inequality:</strong> If <span class="math inline">\(Z\)</span> denotes a random variable with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>, then
<span class="math display">\[P\left(|X-\mu|&gt; \sigma \cdot m\right)\le \frac{1}{m^2}\quad\hbox{ for all }\quad m&gt;0\]</span>
<span class="math display">\[\Rightarrow
P\left(|\hat\theta_n-\theta|&gt; n^{-1/2}\sqrt{C} \cdot
\frac{1}{\sqrt{\epsilon}}\right)\leq \epsilon \quad\hbox{ for all }\quad\epsilon&gt;0\]</span></p>
<p><br />
</p>
<!-- **Generalization:** Let $\hat\theta_n$ be a *not necessarily unbiased* estimator of an unknown parameter $\theta$. If $E\left((\hat\theta_n-\theta)^2\right)=Cn^{-2r}$ for some $0<C<\infty$, then $|\hat\theta_n-\theta|=O_P(n^{-r})$ and $n^{-r}$ is the rate of convergence of $\hat\theta_n$. -->
<!-- \ -->
<p><strong>Example:</strong> Assume an i.i.d. sample <span class="math inline">\(X_1,\dots,X_n\)</span> with mean <span class="math inline">\(\mu=E(X_i)\)</span> and variance <span class="math inline">\(\sigma^2=\textrm{var}(X_i)&lt;\infty\)</span>. The sample mean <span class="math inline">\(\bar X\)</span> (<span class="math inline">\(\equiv \bar X_n\)</span>) is an unbiased estimator of <span class="math inline">\(\mu\)</span> with variance <span class="math inline">\(\textrm{var}(\bar X)=\sigma^2/n\)</span>.
For large <span class="math inline">\(n\)</span> we have by the central limit theorem that approximately <span class="math inline">\(\sqrt{n}(\bar X-\mu)\sim N(0,\sigma^2)\)</span>. Therefore, for example:</p>
<ul>
<li>with <span class="math inline">\(\epsilon=0.05\)</span> we obtain
<span class="math display">\[P\left(|\bar X_n-\mu|\ge 1.96\sigma\cdot n^{-1/2}\right)=0.05\]</span></li>
<li>with <span class="math inline">\(\epsilon=0.01\)</span> we obtain
<span class="math display">\[P\left(|\bar X_n-\mu|\ge 2.64\sigma\cdot n^{-1/2}\right)=0.01.\]</span></li>
</ul>
<p>Generalizing this argument for all possible <span class="math inline">\(\epsilon&gt;0\)</span> we can conclude that
<span class="math inline">\(\bar X-\mu=O_P(n^{-1/2})\)</span>. On the other hand for any <span class="math inline">\(r&gt;1/2\)</span> we have <span class="math inline">\(n^{-r}/n^{-1/2}\rightarrow 0\)</span> as <span class="math inline">\(n\rightarrow \infty\)</span>. Hence, for any constant <span class="math inline">\(c&gt;0\)</span></p>
<p><span class="math display">\[\begin{align*}
 &amp;P\left(|\bar X_n-\mu|\geq c\sigma\cdot n^{-r}\right)=\\
=&amp;P\left(|\bar X_n-\mu|\ge (c\sigma\cdot n^{-1/2})\cdot\frac{n^{-r}}{n^{-1/2}}\right)\rightarrow 1 \quad\text{as}\quad n\rightarrow \infty.
\end{align*}\]</span></p>
<p>Therefore <span class="math inline">\(n^{-1/2}\)</span> is the <strong>rate of convergence</strong> of <span class="math inline">\(\bar X\)</span>.</p>
<p><br />
</p>
<p>Note that:</p>
<ul>
<li>Maximum-likelihood estimators of an unknown parameter usually possess the rate of convergence <span class="math inline">\(n^{-1/2}\)</span> (there are exceptions!).</li>
<li>The situation is different, for instance, in nonparametric curve estimation problems. For example kernel estimators (of a density or regression function) only achieve the rate of convergence <span class="math inline">\(n^{-2/5}\)</span>.</li>
<li>The rate of convergence is an important criterion for selecting the best possible estimator for a given problem. For most parametric problems it is well known that the optimal (i.e.Â fastest possible) convergence rate is <span class="math inline">\(n^{-1/2}\)</span>. In nonparametric regression or density estimation the optimal convergence rate is only <span class="math inline">\(n^{-2/5}\)</span>, if the underlying function is twice continuously differentiable.</li>
</ul>
<p><br />
</p>
<p><span class="math inline">\(O_P\)</span>-rules:</p>
<ul>
<li>We have
<span class="math display">\[Z_n\rightarrow_P Z \qquad \text{if and only if }\qquad Z_n=Z+o_p(1)\]</span>
This follows from <span class="math inline">\(Z_n=Z+(Z_n-Z)\)</span> and <span class="math inline">\(Z_n-Z\rightarrow_P 0\)</span>.</li>
</ul>
<p><br />
</p>
<ul>
<li>If <span class="math inline">\(Z_n=O_P(n^{-\delta})\)</span> for some <span class="math inline">\(\delta&gt;0\)</span>, then <span class="math inline">\(Z_n=o_P(1)\)</span></li>
</ul>
<p><br />
</p>
<ul>
<li>If <span class="math inline">\(Z_n=O_P(r_n)\)</span>, then <span class="math inline">\(Z_n^\delta=O_P(r_n^\delta)\)</span> for any <span class="math inline">\(\delta&gt;0\)</span>. Similarly,
<span class="math inline">\(Z_n=o_P(r_n)\)</span> implies <span class="math inline">\(Z_n^\delta=o_P(r_n^\delta)\)</span> for any <span class="math inline">\(\delta&gt;0\)</span>.</li>
</ul>
<p><br />
</p>
<ul>
<li>If <span class="math inline">\(Z_n=O_P(r_n)\)</span> and <span class="math inline">\(V_n=O_P(s_n)\)</span>, then
<span class="math display">\[\begin{align*}
Z_n+V_n &amp; =O_P(\max\{r_n,s_n\})\\
Z_nV_n &amp;  =O_P(r_ns_n)
\end{align*}\]</span></li>
</ul>
<p><br />
</p>
<ul>
<li>If <span class="math inline">\(Z_n=o_P(r_n)\)</span> and <span class="math inline">\(V_n=O_P(s_n)\)</span>, then <span class="math inline">\(Z_nV_n=o_P(r_n s_n)\)</span></li>
</ul>
<p><br />
</p>
<ul>
<li>If <span class="math inline">\(E(|Z_n|^k)=O(r_n)\)</span>, then <span class="math inline">\(Z_n=O_p(|r_n|^{1/k})\)</span> for <span class="math inline">\(k=1,2,3,\dots\)</span></li>
</ul>
</div>
<div id="asymptotic-distributions" class="section level2" number="2.4">
<h2><span class="header-section-number">2.4</span> Asymptotic Distributions</h2>
<p>The practically most important version of stochastic convergence is convergence in distribution. Knowledge about the âasymptotic distributionâ of an estimator allows to construct confidence intervals and tests.</p>
<p><br />
</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(Z_n\)</span> be a sequence of random variables with corresponding distribution functions <span class="math inline">\(G_n\)</span>. Then <span class="math inline">\(Z_n\)</span> <strong>converges in distribution</strong> to a random variable <span class="math inline">\(Z\)</span> with distribution function <span class="math inline">\(G\)</span>, if
<span class="math display">\[G_n(x)\to G(x)\quad\hbox{ as }\quad n\to\infty \]</span>
at all continuity points <span class="math inline">\(x\)</span> of <span class="math inline">\(G\)</span> (abbreviated: <span class="math inline">\(Z_n\to_L Z\)</span> or <span class="math inline">\(Z_n\to_L G\)</span> or â<span class="math inline">\(\to_D\)</span>â instead of â<span class="math inline">\(\to_L\)</span>â).</p>
<p><br />
</p>
<p>In a vast majority of practically important situation the limiting distribution is the normal distribution. One then speaks of <strong>asymptotic normality</strong>. Asymptotic normality is usually a consequence of <a href="https://www.statlect.com/asymptotic-theory/central-limit-theorem%5D">central limit theorems</a>. The simplest result in this direction is the central limit theorem of Lindeberg-Levy.</p>
<p><br />
</p>
<p><strong>Theorem (Lindeberg-Levy)</strong> Let <span class="math inline">\(Z_1,Z_2,\dots\)</span> be a sequence of i.i.d. random variables with
finite mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2&lt;\infty\)</span>. Then
<span class="math display">\[\sqrt{n}\left(\frac{1}{n} \sum_{i=1}^n Z_i -\mu\right)\rightarrow_L N(0,\sigma^2).\]</span></p>
<p><br />
</p>
<p><strong>Example:</strong> Let <span class="math inline">\(X_1,\dots,X_n\)</span> be independent random variables
with <span class="math inline">\(E(X_i)=\mu\)</span>, <span class="math inline">\(Var(X_i)=\sigma^2\)</span>. Then the central limit theorem of Lindeberg-Levy implies that
<span class="math display">\[\sqrt{n}(\bar X -\mu ) \to_L N(0,\sigma^2)\quad\text{ or equivalently }\quad
\frac{\sqrt{n}(\bar X -\mu )}{\sigma}\to_L N(0,1).\]</span>
We can conclude that <span class="math inline">\(\bar X\)</span> is an âasymptotically normal estimatorâ of <span class="math inline">\(\mu\)</span>. If <span class="math inline">\(n\)</span> is sufficiently large, then <span class="math inline">\(\bar X\)</span> is approximatively normal with mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2/n\)</span>. Frequently used notations:</p>
<ul>
<li><span class="math inline">\(\bar X\sim AN(\mu,\sigma^2/n)\)</span></li>
<li><span class="math inline">\(\bar X\overset{a}{\sim}N(\mu,\sigma^2/n)\)</span></li>
</ul>
<p><br />
</p>
<p>Most estimators <span class="math inline">\(\hat\theta_n\)</span> used in parametric and nonparametric statistics are asymptotically normal. In parametric problems (with rate of convergence <span class="math inline">\(n^{-1/2}\)</span>) one usually obtains
<span class="math display">\[\sqrt{n}(\hat\theta_n -\theta )\to_L N(0,v^2),\]</span>
where <span class="math inline">\(v^2\)</span> is the asymptotic variance of the estimator (often, but not necessarily, <span class="math inline">\(v^2=\lim_{n\to\infty} n\cdot\textrm{var}(\hat\theta_n)\)</span>).</p>
<!-- $$\Rightarrow \hat\theta_n\sim AN(\theta,v^2/n)$$ -->
<p><br />
</p>
<p><strong>Multivariate generalization:</strong> The above concepts are easily generalized to estimators <span class="math inline">\(\hat\theta_n\)</span> of a multivariate parameter vector <span class="math inline">\(\theta\in\mathbb{R}^p\)</span>. Consistency and rates of convergence then have to be derived separately for each element of the vector. Convergence in distribution is defined via convergence of the multivariate distribution functions. For standard estimators (e.g., maximum likelihood) in parametric problems one usually obtains
<span class="math display">\[\sqrt{n}(\hat\theta_n -\theta )\to_L N_p(0,V),\]</span>
where <span class="math inline">\(V\)</span> is the asymptotic covariance matrix (usually, <span class="math inline">\(V=\lim_{n\to\infty} n\cdot\textrm{Cov}(\hat\theta_n)\)</span>).</p>
<p><br />
</p>
<p>Multivariate normality holds if and only if for any vector <span class="math inline">\(c=(c_1,\dots,c_p)&#39;\in\mathbb{R}^p\)</span> with <span class="math inline">\(\sum_{j=1}^p c_j^2=\Vert c\Vert_2^2=1\)</span>
<span class="math display">\[\sqrt{n}\left(\sum_{j=1}^p c_j (\hat\theta_{jn} -\theta_j)\right)=\sqrt{n}\left(c&#39;\hat\theta_n-c&#39;\theta\right)\to_L N\left(0,v_c^2\right),\]</span>
where
<span class="math display">\[v_c^2=c&#39;Vc=\sum_{j=1}^p\sum_{k=1}^p c_jc_k V_{jk},\]</span>
and where <span class="math inline">\(V_{jk}\)</span> are the elements of the asymptotic covariance matrix <span class="math inline">\(V\)</span>.</p>
<p>This condition is frequently called <strong>âCramer-Wold deviceâ</strong>. Using one-dimensional central limit theorems it can be verified for any vector <span class="math inline">\(c\)</span>.</p>
<p><br />
</p>
<p><strong>Example:</strong> Let <span class="math inline">\(X_1=(X_{11},X_{12})&#39;,\dots,X_n=(X_{n1},X_{n2})&#39;\)</span> be i.i.d. two-dimensional random vectors with <span class="math inline">\(E(X_i)=\mu=(\mu_1,\mu_2)&#39;\)</span> and <span class="math inline">\(Cov(X_i)=\Sigma\)</span>. The Cramer-Wold device and Lindeberg-Levyâs central limit theorem then imply that
<span class="math display">\[\sqrt{n}\left(\bar X -\mu\right)\to_L N_2\left(0,\Sigma\right).\]</span></p>
<p><br />
</p>
<p>Note that asymptotic normality usually also holds for nonparametric curve estimators with convergence rates slower than <span class="math inline">\(n^{-1/2}\)</span>.</p>
</div>
<div id="asymptotic-theory" class="section level2" number="2.5">
<h2><span class="header-section-number">2.5</span> Asymptotic Theory</h2>
<p>Many estimation procedures in modern statistics rely on fairly general assumptions. For a given sample size <span class="math inline">\(n\)</span> it is then often impossible to derive the exact distribution of <span class="math inline">\(\theta_n\)</span>. Necessary calculations are too complex, and finite sample distributions usually depend on unknown characteristics of the distribution of the underlying data.</p>
<p><br />
</p>
<p>The goal of asymptotic theory then is to derive reasonable approximations. For <strong>large samples</strong> such approximations are of course very accurate, for <strong>small samples</strong> there may exist a considerable approximation error. Therefore, for small samples the approximation quality of asymptotic approximations is usually studied by Monte-Carlo approximations.</p>
<p><br />
</p>
<p>Asymptotic theory is used in order to select an appropriate estimation procedure in complex situations. The idea is to determine the estimator which, at least for large sample sizes, provides the smallest possible estimation error. This leads to the concept of âasymptotically efficientâ estimators.</p>
<p><br />
</p>
<p>Properties of an asymptotically efficient estimator <span class="math inline">\(\theta_n\)</span>:</p>
<ul>
<li>For the estimation problem to be considered <span class="math inline">\(\theta_n\)</span> is consistent and adopts the fastest possible rate of convergence
(generally: <span class="math inline">\(n^{-1/2}\)</span> in parametric statistics; <span class="math inline">\(n^{-2/5}\)</span> can be achieved in nonparametric univariate curve estimation problems).</li>
<li>In most regular situations one is additionally interested in a âbest asymptotically normalâ (BAN) estimator. Assume that <span class="math inline">\(\sqrt{n}(\theta_n -\theta)\sim N(0,v^2)\)</span>. Then <span class="math inline">\(\theta_n\)</span> is a BAN-estimator if any alternative estimator <span class="math inline">\(\tilde\theta_n\)</span> with
<span class="math inline">\(\sqrt{n}(\tilde\theta_n -\theta)\sim N(0,\tilde v^2)\)</span> possesses a larger asymptotic variance, i.e.Â <span class="math inline">\(\tilde v^2\geq v^2\)</span>.</li>
<li><strong>Multivariate generalization:</strong> An estimator <span class="math inline">\(\theta_n\)</span> with <span class="math inline">\(\sqrt{n}(\theta_n -\theta)\sim N_p(0,V)\)</span> is best asymptotically normal if
<span class="math display">\[c&#39;\tilde V c\geq c&#39;Vc\quad\hbox{ for all }\quad c\in\mathbb{R}^p, \Vert c\Vert_2^2=1\]</span>
for any other estimator <span class="math inline">\(\tilde\theta_n\)</span> satisfying <span class="math inline">\(\sqrt{n}(\tilde\theta_n -\theta)\sim N_p(0,\tilde V)\)</span>.</li>
</ul>
<p><br />
</p>
<p>For most estimation problems in parametric statistics maximum-likelihood estimators are best asymptotically normal.</p>
</div>
<div id="mathematical-tools" class="section level2" number="2.6">
<h2><span class="header-section-number">2.6</span> Mathematical tools</h2>
<div id="taylor-expansions" class="section level3" number="2.6.1">
<h3><span class="header-section-number">2.6.1</span> Taylor expansions</h3>
<p>Taylorsâ theorem: Let <span class="math inline">\(f\)</span> be a real-valued function which is
<span class="math inline">\(k+1\)</span> continuously differentiable in the interior of an interval <span class="math inline">\([a,b]\)</span>. Consider a point
<span class="math inline">\(x_0\in (a,b)\)</span>. For any other value <span class="math inline">\(x\in (a,b)\)</span> there exists some <span class="math inline">\(\psi\in [x_0,x]\)</span> such that
<span class="math display">\[f(x)=f(x_0)+\sum_{r=1}^k \frac{1}{r!}f^{(r)}(x_0)\cdot(x-x_0)^r+\frac{1}{(k+1)!}f^{(k+1)}(\psi)\cdot(x-x_0)^{k+1}\]</span><br />
</p>
<p>Qualitative version of Taylorsâ formula:
<span class="math display">\[f(x)=f(x_0)+\sum_{r=1}^k \frac{1}{r!}f^{(r)}(x_0)\cdot(x-x_0)^r+O((x-x_0)^{k+1})\]</span></p>
<p><br />
</p>
<p><strong>Example:</strong> Let <span class="math inline">\(f(x)=ln(x)\)</span> und <span class="math inline">\(x_0=1\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f&#39;(x_0)=1\)</span>, <span class="math inline">\(f&#39;&#39;(x_0)=-1\)</span>.</p>
<p><br />
</p>
<p>First order Taylor approximation: <span class="math inline">\(f(x)=\tilde f(x)+O((x-x_0)^{2})\)</span>, where <span class="math inline">\(\tilde f(x)=x-x_0\)</span></p>
<ul>
<li><span class="math inline">\(x=1.05\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.04879\)</span>, <span class="math inline">\(\tilde f(x)=0.05\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.00121\)</span></li>
<li><span class="math inline">\(x=1.1\phantom{0}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.09531\)</span>, <span class="math inline">\(\tilde f(x)=0.1\phantom{0}\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.00469\)</span></li>
<li><span class="math inline">\(x=1.5\phantom{0}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.40546\)</span>, <span class="math inline">\(\tilde f(x)=0.5\phantom{0}\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.09454\)</span></li>
<li><span class="math inline">\(x=2\phantom{.00}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.69315\)</span>, <span class="math inline">\(\tilde f(x)=1\phantom{.00}\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.30685\)</span></li>
</ul>
<p><br />
</p>
<p>Second order Taylor approximation: <span class="math inline">\(f(x)=\tilde f(x)+O((x-x_0)^{3})\)</span>, where <span class="math inline">\(\tilde f(x)=x-x_0-\frac{1}{2} (x-x_0)^2\)</span></p>
<ul>
<li><span class="math inline">\(x=1.05\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.04879\)</span>, <span class="math inline">\(\tilde f(x)=0.04875\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.00004\)</span></li>
<li><span class="math inline">\(x=1.1\phantom{0}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.09531\)</span>, <span class="math inline">\(\tilde f(x)=0.95\phantom{000}\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.00031\)</span></li>
<li><span class="math inline">\(x=1.5\phantom{0}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.40546\)</span>, <span class="math inline">\(\tilde f(x)=0.375\phantom{00}\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.03046\)</span></li>
<li><span class="math inline">\(x=2\phantom{.00}\)</span> <span class="math inline">\(\Rightarrow\)</span> <span class="math inline">\(f(x)=0.69315\)</span>, <span class="math inline">\(\tilde f(x)=0.5\phantom{0000}\)</span> and <span class="math inline">\(|f(x)-\tilde f(x)|=0.19315\)</span></li>
</ul>
<p><br />
</p>
<p><strong>Multivariate generalization:</strong> <span class="math inline">\(x_0,x\in\mathbb{R}^p\)</span>, <span class="math inline">\(f&#39;(x_0)\in\mathbb{R}^p\)</span>, <span class="math inline">\(f&#39;&#39;(x_0)\)</span> a <span class="math inline">\(p\times p\)</span> Matrix.</p>
<p>First order Taylor approximation:
<span class="math display">\[f(x)=f(x_0)+f&#39;(x_0)\cdot(x-x_0)+O(\Vert x-x_0\Vert_2^2)\]</span></p>
<p>Second order Taylor approximation:
<span class="math display">\[f(x)=f(x_0)+f&#39;(x_0)\cdot(x-x_0)+\frac{1}{2} (x-x_0)^T f&#39;&#39;(x_0)(x-x_0)+O(\Vert x-x_0\Vert_2^3)\]</span></p>
</div>
<div id="tools-for-deriving-asymptotic-distributions" class="section level3" number="2.6.2">
<h3><span class="header-section-number">2.6.2</span> Tools for deriving asymptotic distributions</h3>
<p>Let <span class="math inline">\(\{W_n\}\)</span>, <span class="math inline">\(\{Z_n\}\)</span> be sequences of random variables, then:</p>
<ul>
<li><span class="math inline">\(Z_n=W_n+o_P(1)\quad \Leftrightarrow \quad Z_n-W_n\to_P 0\)</span>. If additionally <span class="math inline">\(W_n\to_L N(0,v^2)\)</span> then <span class="math inline">\(Z_n\to_L N(0,v^2)\)</span>.</li>
<li>For any fixed constant <span class="math inline">\(c\neq 0\)</span>: If <span class="math inline">\(Z_n\to_P c\)</span> and <span class="math inline">\(W_n\to_L N(0,v^2)\)</span>, then
<span class="math display">\[cW_n\to_L N(0,c^2v^2)\quad\hbox{as well as }\quad  V_n:=Z_n\cdot W_n\to_L N(0,c^2v^2).\]</span>
Furthermore, If <span class="math inline">\(Z_n\)</span> and <span class="math inline">\(c\)</span> are positive (with probability 1) then also
<span class="math display">\[W_n/c\to_L N(0,v^2/c^2)\quad\hbox{as well as }\quad  V_n:= W_n/Z_n\to_L N(0,v^2/c^2).\]</span></li>
<li>Multivariate generalization (<span class="math inline">\(C\)</span>, <span class="math inline">\(Z_n\)</span> <span class="math inline">\(p\times p\)</span> matrices; <span class="math inline">\(W_n\)</span> <span class="math inline">\(p\)</span>-dimensional random vectors):
If <span class="math inline">\(Z_n\to_P C\)</span> as well as <span class="math inline">\(W_n\to_L N_p(0,V)\)</span>, then
<span class="math display">\[\begin{align*}
CW_n&amp;\to_L N_p(0,CVC&#39;)\quad\hbox{as well as }\\
V_n:=Z_n\cdot W_n&amp;\to_L N_p(0,CVC&#39;)
\end{align*}\]</span></li>
</ul>
<!-- For any fixed constant  $c\neq 0$: $W_n\to_L N(0,v^2)$ implies $cW_n\to_L N(0,c^2v^2)$ -->
</div>
<div id="the-delta-method" class="section level3" number="2.6.3">
<h3><span class="header-section-number">2.6.3</span> The Delta-Method</h3>
<p>A further tool which is frequently used in asymptotic statistics is the so-called delta-method.</p>
<p><strong>Delta-Method:</strong> Let <span class="math inline">\(\widehat{\theta}_n\)</span> be a sequence of estimators of a one-dimensional parameter <span class="math inline">\(\theta\)</span> satisfying
<span class="math inline">\(n^{r} (\widehat{\theta}_n-\theta)\rightarrow_L N(0,v^2),\)</span> and let <span class="math inline">\(g(.)\)</span> be a real-valued function which is continuously differentiable at <span class="math inline">\(\theta\)</span> and satisfies <span class="math inline">\(g&#39;(\theta)\neq 0\)</span>. Then
<span class="math display">\[n^{r} \left(g(\widehat{\theta}_n)-g(\theta)\right) \rightarrow_L  N\left(0,g&#39;(\theta)^2v^2\right).\]</span></p>
<p><br />
</p>
<p><strong>Example:</strong> Assume an i.i.d. random sample <span class="math inline">\(X_1,\dots,X_n\)</span> from an exponential distribution. That is, the underlying density of <span class="math inline">\(X_i\)</span>, <span class="math inline">\(i=1,\dots,n\)</span>, is given by <span class="math inline">\(f(x|\theta)=\theta\exp(-\theta x)\)</span>. We then have <span class="math inline">\(\mu:=E(X_i)=1/\theta\)</span> as well as <span class="math inline">\(\sigma^2_X:=\textrm{var}(X_i)=1/\theta^2\)</span>. The underlying parameter <span class="math inline">\(\theta&gt;0\)</span> is unknown and has to be estimated from the data.</p>
<p><br />
</p>
<p>The maximum-likelihood estimator of <span class="math inline">\(\theta\)</span> is <span class="math inline">\(\hat\theta=1/\bar X\)</span>.</p>
<p><br />
</p>
<p>We know that <span class="math inline">\(\sqrt{n}(\bar X-\frac{1}{\theta})\to_L N(0,\frac{1}{\theta^2})\)</span>, but whatâs about the distribution of <span class="math inline">\(1/\bar X\)</span>? For this purpose the delta-method can be applied with <span class="math inline">\(g(x)=1/x\)</span>. Then <span class="math inline">\(g&#39;(x)=-1/x^2\)</span>, <span class="math inline">\(g&#39;(1/\theta)=-\theta^2\)</span>, and consequently
<span class="math display">\[n^{1/2} \left(\frac{1}{\bar X}-\theta\right)=n^{1/2}\left(g\left(\bar X\right)-g\left(\frac{1}{\theta}\right)\right)\rightarrow_L N\left(0,\theta^2\right).\]</span></p>
<!-- ## Example: Simple Linear Regression -->
<!-- Assume an i.i.d. sample $(Y_1,X_1),\dots,(Y_n,X_n)$ of random variables, and consider estimating the slope parameters in the linear regression model -->
<!-- $$Y_i=\alpha+\beta X_i+\epsilon_i,\quad, i=1,\dots,n,$$ -->
<!-- where $\epsilon_i$ are i.i.d. zero mean error terms with variance $\sigma_\epsilon^2$ (independent of $X_i$). Furthermore assume that $\textrm{var}(X_i)=\sigma_X^2$. -->
<!-- \ -->
<!-- Observe that  -->
<!-- \begin{align*} -->
<!-- (Y_i-\bar{Y})&=\beta (X_i - \bar{X}) + (\epsilon_i - \bar{\epsilon}) -->
<!-- \end{align*} -->
<!-- \ -->
<!-- The least-squares estimator $\hat\beta$ of $\beta$ is given by -->
<!-- \begin{align*} -->
<!-- \hat\beta -->
<!-- &=\frac{\sum_{i=1}^n (X_i-\bar X)Y_i}{\sum_{i=1}^n (X_i-\bar X)^2}\\ -->
<!-- &=\beta+\frac{\sum_{i=1}^n (X_i-\bar X)(\epsilon_i-\bar\epsilon)}{\sum_{i=1}^n (X_i-\bar X)^2}\\ -->
<!-- &=\beta+\frac{\sum_{i=1}^n (X_i-\bar X)\epsilon_i}{\sum_{i=1}^n (X_i-\bar X)^2} -->
<!--        -\frac{\sum_{i=1}^n (X_i-\bar X)\bar\epsilon}{\sum_{i=1}^n (X_i-\bar X)^2} -->
<!-- \end{align*} -->
<!-- \ -->
<!-- We already know that $\bar X -\mu_x=O_P(n^{-1/2})$, where $\mu_x=E(X_i)$ is the mean of $X_i$. Also note that the mean of $\epsilon_i$ is zero, and hence $\bar \epsilon=O_P(n^{-1/2})$. Therefore, $(\bar X -\mu_x)\bar\epsilon=O_P(n^{-1})$. -->
<!-- \ -->
<!-- Furthermore, it follows from the law of large numbers that -->
<!-- $$\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2 \to_P \sigma_x^2.$$ -->
<!-- \ -->
<!-- Now, -->
<!-- $$\hat\beta-\beta = -->
<!-- \frac{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)\epsilon_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2} -->
<!-- =\frac{\frac{1}{n}\sum_{i=1}^n (X_i-\mu)\epsilon_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2} -->
<!-- +\underbrace{\frac{ (\mu-\bar X)\bar\epsilon}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2}}_{O_P(n^{-1})}$$ -->
<!-- \ -->
<!-- This implies that -->
<!-- $$\sqrt{n}(\hat\beta-\beta)  -->
<!-- =\frac{\sqrt{n}\frac{1}{n}\sum_{i=1}^n (X_i-\mu)\epsilon_i}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2} -->
<!-- +\underbrace{\frac{\sqrt{n} (\mu-\bar X)\bar\epsilon}{\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2}}_{o_P(1)}$$ -->
<!-- \ -->
<!-- By the above rules one can neglect the additional term of order $o_P(1)$ when deriving the asymptotic distribution. Note that $(X_i-\mu)\epsilon_i$ are i.i.d. random variables with zero mean and variance $\sigma_X^2\sigma_\epsilon^2$. The central limit theorem of Lindeberg-Levy thus implies that -->
<!-- $$\sqrt{n}\frac{1}{n}\sum_{i=1}^n (X_i-\mu)\epsilon_i\to_L N(0,\sigma_x^2\sigma_\epsilon^2).$$ -->
<!-- \ -->
<!-- Since $\frac{1}{n}\sum_{i=1}^n (X_i-\bar X)^2 \to_P \sigma_x^2$ we can conclude by the above rules that -->
<!-- $$\sqrt{n}(\hat\beta-\beta) \to_L N(0,\frac{\sigma_\epsilon^2}{\sigma_X^2}).$$ -->
<!-- \subsection{ Asymptotic theory of  maximum-likelihood estimators} -->
<!-- \bigbreak\noindent -->
<!-- We only consider the simplest situation: Assume an i.i.d. sample $X_1,\dots,X_n$, and suppose that -->
<!-- the distribution -->
<!-- of $X_i$ possesses a density $f(x,\tht)$. The true parameter -->
<!--  $\tht\in \R$ is unknown (example: density of an exponential distribution -->
<!-- $f(x|\tht)=\tht\exp(- \tht x)$) -->
<!-- \bigbreak\noindent -->
<!-- \begin{itemize} -->
<!-- \item  Likelihood function: -->
<!-- $$L(\tht)=\prod_{i=1}^n f(X_i,\tht)$$ -->
<!-- \item  Log-likelihood function: -->
<!-- $$l(\tht)=\log L(\tht)=\sum_{i=1}^n \log f(X_i,\tht)$$ -->
<!-- \item The maximum-likelihood estimator $\thth_n$ maximizes  $l(\tht)$ -->
<!-- \end{itemize} -->
<!-- It can generally be shown that $\tht_n$ is a  consistent estimator of -->
<!-- $\tht$. Derivation of the asymptotic distribution relies on a  Taylor expansion of the derivative  -->
<!-- $l'(\cdot)$ of the log-likelihood function. For some $\psi_n$ in between  $\thth_n$ and -->
<!-- $\tht$ we have -->
<!-- $$l'(\thth)=l'(\tht)+l''(\psi)(\thth_n-\tht)$$ -->
<!-- Since $\thth_n$ maximizes the log-Likelihood function  it follows that -->
<!-- $l'(\thth)=0$. This implies -->
<!-- $$l'(\tht)=-l''(\psi_n)(\thth_n-\tht)$$ -->
<!-- Note that necessarily -->
<!-- $\int_{-\infty}^{\infty} f(x|\vartheta)dx=1$ for all possible values of the true parameter -->
<!--  $\vartheta$. Therefore, -->
<!-- $\int_{-\infty}^{\infty} \frac{\partial}{\partial \tht}f(x|\theta)dx=0$ and -->
<!-- $\int_{-\infty}^{\infty} \frac{\partial^2}{\partial \tht^2}f(x|\theta)dx=0$. -->
<!-- It follows that -->
<!-- $$E(l'(\thth))=n E(\frac{\partial}{\partial \tht} \log f(X_i,\tht)) -->
<!-- =\int_{-\infty}^{\infty} \frac{\frac{\partial}{\partial \tht}  f(x,\tht)} -->
<!-- {f(x,\tht)}f(x,\tht)dx=0$$ -->
<!-- In addition one obtains -->
<!-- $$Var(l'(\thth))=n Var(\frac{\partial}{\partial \tht} \log f(X_i,\tht)) -->
<!-- =nE( (\frac{\frac{\partial}{\partial \tht}  f(X_i,\tht)} -->
<!-- {f(X_i,\tht)})^2)$$ -->
<!-- Define $j(\tht):=E( (\frac{\frac{\partial}{\partial \tht}  f(X_i,\tht)} -->
<!-- {f(X_i,\tht)})^2)$. By the central limit theorem we now have -->
<!-- $$\frac{l'(\tht)}{\sqrt{n \cdot j(\tht)} } \ra_L N(0,1)$$ -->
<!-- and thus -->
<!-- $$\frac{-l''(\psi_n)}{\sqrt{n \cdot j(\tht)}}(\thth_n-\tht) \ra_L N(0,1)$$ -->
<!-- Further analysis requires to study the  term $l''(\psi_n)$. -->
<!-- $$\frac{1}{n}E(l''(\tht))= -->
<!-- E\left( \frac{\frac{\partial^2}{\partial \tht^2}  f(X_i,\tht)} -->
<!-- {f(X_i,\tht)}-( \frac{\frac{\partial}{\partial \tht}  f(X_i,\tht)} -->
<!-- {f(X_i,\tht)})^2\right)=-j(\tht)$$ -->
<!-- $$Var(\frac{1}{n}l''(\tht))=\frac{1}{n} -->
<!-- Var(\frac{\partial^2}{\partial \tht^2}  \log f(X_i,\tht))\ra_{n\ra\infty} 0$$ -->
<!-- $$\Rightarrow \frac{1}{n}l''(\tht)\ra_P -j(\tht)\quad \hbox{ as   } n\ra\infty$$ -->
<!-- Since $\psi_n\ra_P \tht$, we arrive at -->
<!-- $$\frac{1}{n}l''(\psi_n)\ra_P -j(\tht)\quad \hbox{ as } n\ra\infty$$ -->
<!-- and -->
<!-- $$n^{-1/2}\frac{-l''(\psi_n)}{\sqrt{n \cdot j(\tht)}} -->
<!-- =\frac{-\frac{1}{n}l''(\psi_n)}{\sqrt{ j(\tht)}}\ra_P \sqrt{ j(\tht)}$$ -->
<!-- Since $\frac{-l''(\psi_n)}{\sqrt{n \cdot j(\tht)}}(\thth_n-\tht)= -->
<!-- n^{-1/2}\frac{-l''(\psi_n)}{\sqrt{n \cdot j(\tht)}}\cdot n^{1/2}(\thth_n-\tht)$ -->
<!-- we can conclude that -->
<!-- $$\sqrt{ j(\tht)}n^{1/2}(\thth_n-\tht)\ra_L N(0,1),$$ -->
<!-- or equivalently -->
<!-- $$\thth_n-\tht\sim AN(0,\frac{1}{n j(\tht)})$$ -->
<!-- with $n j(\tht)=-E(l''(\tht))$. -->
<!-- The above arguments can easily be generalized to multidimensional parameter vectors $\tht\in\R^p$. $j(\tht)$ then becomes a $p\times p$ matrix, and -->
<!-- $$\thth_n-\tht\sim AN_p(0,\frac{1}{n} j(\tht)^{-1}). $$ -->
<!-- $j(\tht)^{-1}$ is called ``Fisher information matrix''. -->
<!-- \bigbreak -->
<!-- {\bf Example:} Assume an i.i.d. sample $X_1,\dots,X_n$ from an exponential distribution, i.e. the underlying density of $X_i$ is given by $f(x|\tht)=\tht\exp(-\tht x)$. We then have $\mu:=E(X_i)=\frac{1}{\tht}$ as well as $\sigma^2_X:=\textrm{var}(X_i)=\frac{1}{\tht^2}$. The -->
<!-- log-likelihood functions is given by  -->
<!-- $$l(\tht)=\sum_{i=1}^n \log (\tht\exp(-\tht X_i)))=n \log \tht -\sum_{i=1}^n \tht X_i$$ -->
<!-- $$\Rightarrow \quad l'(\tht)=n\frac{1}{\tht} + \sum_{i=1}^n X_i.$$ -->
<!-- As already mentioned above, the maximum-likelihood estimator of $\tht$ then is $\hat\tht=\frac{1}{\bar X}$. -->
<!-- Inference may then be based on likelihood-theory. We have -->
<!-- $$j(\tht)=-\frac{1}{n}E(l''(\tht))=\frac{1}{\tht^2},$$ -->
<!-- and by the above theorem -->
<!-- $$\frac{1}{\bar X}-\tht\sim AN(0,\frac{1}{n j(\tht)})=AN(0,\frac{\tht^2}{n}).$$ -->
<!-- This obviously coincides with the result obtained by the delta-method. -->

</div>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="introduction-to-r.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="statistical-hypothesis-testing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/clipboard.min.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-clipboard.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": false,
"facebook": true,
"twitter": true,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"whatsapp": false,
"all": ["facebook", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"view": {
"link": null,
"text": null
},
"download": ["RM_ES_Script.pdf"],
"search": {
"engine": "fuse",
"options": null
},
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
